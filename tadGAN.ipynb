{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras from tensorflow\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1' \n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tadgan_example import TadGAN\n",
    "from tqdm import trange\n",
    "import sys\n",
    "import tensorflow.python.ops.numpy_ops.np_config as np_config\n",
    "np_config.enable_numpy_behavior()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>function for wasserstein loss</h1>\n",
    "- described in the [Wasserstein GAN](https://arxiv.org/abs/1701.07875)\n",
    "<br>\n",
    "- implemented in [tadGAN](https://arxiv.org/pdf/2009.07769.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _wasserstein_loss(y_true, y_pred):\n",
    "    return tf.keras.backend.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define all of the input shapes and parameters</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100 # sequence length must always be even\n",
    "ts_input_shape: Tuple[int] = (seq_len, 8)#this is for univariant data , change the shape accorindly ex for multivariant data change to (100,num_features)\n",
    "latent_dim: int = 20 #latent dimension where encoder and decoder will be trained\n",
    "gradient_penalty_weight: int = 10#gradient penelty weight for wasserstein loss\n",
    "n_iterations_critic: int = 5#number of iterations for training the critic per iter for encoder and decoder\n",
    "\n",
    "# sub network hyper parameters\n",
    "encoder_lstm_units: int = 100 # number of units in encoder LSTM\n",
    "generator_lstm_units: int = 100 # number of units in generator LSTM\n",
    "generator_output_activation: str = \"tanh\" # activation function for generator output\n",
    "critic_x_cnn_blocks: int = 4 # number of convolutional blocks in critic x\n",
    "critic_x_cnn_filters: int = 64 # number of filters in each convolutional block in critic x\n",
    "critic_z_dense_units: int = 100 # number of units in critic z dense layer\n",
    "\n",
    "log_all_losses: bool = True\n",
    "print_model_summaries: bool = True \n",
    "\n",
    "critic_x_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "critic_z_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "encoder_generator_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "encoder_generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "critic_x_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "critic_z_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Make an encoder layer </h1>\n",
    "    - Build the Encoder subnetwork for the GAN. This model learns the compressed representation of the input and transforms the timesries sequence into the latent space\n",
    "    </br>\n",
    "    - The encoder uses a single layer BI-LSTM network to learn the compressed representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_encoder(input_shape: Tuple[int]=(100,1), lstm_units:int = 100, latent_dim:int=20)->tf.keras.Model:\n",
    "    \"\"\"\n",
    "        The number of LSTM units can be adjusted.\n",
    "\n",
    "        :param lstm_units: Number of LSTM units that could be used for the time series encoding\n",
    "\n",
    "        :input_shape: Tuple of input shape (batch_size, len_sequence, num_features)\n",
    "\n",
    "        :return: Encoder model\n",
    "    \"\"\"\n",
    "\n",
    "    input = tf.keras.layers.Input(shape=input_shape , name=\"encoder_input\")\n",
    "    #create a bi-directional LSTM layer\n",
    "    encoded = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=lstm_units, return_sequences=True))(input)\n",
    "    encoded = tf.keras.layers.Flatten()(encoded)\n",
    "    encoded = tf.keras.layers.Dense(units=latent_dim, name=\"latent_encoding\")(encoded)\n",
    "    encoded = tf.keras.layers.Reshape(target_shape=(latent_dim, 1) , name=\"output_encoder\")(encoded)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=encoded, name=\"encoder\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Make an generator layer </h1>\n",
    "    - Build the generator subnetwork for the GAN. This recreates the timeseries sequence from the latent space\n",
    "    </br>\n",
    "    - The generator  uses a double  layer BI-LSTM network to recreate the timeseries data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator(latent_shape: Tuple[int], lstm_units:int = 64, activation_function:str=\"tanh\") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "        The number of LSTM units can be adjusted.\n",
    "\n",
    "        :param lstm_units: Number of LSTM units that could be used for the time series generation\n",
    "\n",
    "        :param latent_shape: Shape of the latent encoding\n",
    "\n",
    "        :param activation_function: final activation layer for the generator \n",
    "\n",
    "        :return: Generator model\n",
    "    \"\"\"\n",
    "\n",
    "    input = tf.keras.layers.Input(shape=latent_shape, name=\"generator_input\")\n",
    "    decoded = tf.keras.layers.Flatten()(input)\n",
    "\n",
    "    #first layer should be half the size of the sequence\n",
    "    half_seq_length = seq_len // 2\n",
    "    decoded = tf.keras.layers.Dense(units=half_seq_length)(decoded)\n",
    "    decoded = tf.keras.layers.Reshape(target_shape=(half_seq_length, 1))(decoded)  \n",
    "\n",
    "    # generate a new timeseries using two ltsm layers that have 64 hidden units with upsampling  between them\n",
    "    decoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True, dropout=0.2 , recurrent_dropout=0.2), merge_mode=\"concat\")(decoded)\n",
    "    decoder = tf.keras.layers.UpSampling1D(size=2)(decoder)\n",
    "    decoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True, dropout=0.2 , recurrent_dropout=0.2), merge_mode=\"concat\")(decoder)\n",
    "\n",
    "    #rebuild the original shape of the time series for all signals\n",
    "    decoder = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(ts_input_shape[1]))(decoder)\n",
    "    decoder = tf.keras.layers.Activation(activation_function)(decoder)\n",
    "    return tf.keras.Model(inputs=input, outputs=decoder, name=\"generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Make an critic_x  layer </h1>\n",
    "    - Build the critic subnetwork for the GAN. This distinguishes between the timeseries sequence and the generated sequence.\n",
    "    </br>\n",
    "    - The critic uses sequence of 1d convolutional layers to distinguish between the timeseries and generated sequence. and finally a fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_critic_x(input_shape ,num_filters: int = 64, num_cnn_blocks: int = 4) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "        Builds the critic model for the critic_x\n",
    "\n",
    "        :param num_filters: Number of filters in each convolutional block\n",
    "\n",
    "        :param num_cnn_blocks: Number of convolutional blocks in the critic\n",
    "\n",
    "        :return: Critic model\n",
    "    print(input_shape)\n",
    "    \"\"\"\n",
    "    input = tf.keras.layers.Input(shape=input_shape, name=\"critic_x_input\")\n",
    "    #create a convolutional layer with num_filters filters\n",
    "    conv = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=5)(input)\n",
    "    conv = tf.keras.layers.LeakyReLU(alpha=0.2)(conv)\n",
    "    conv = tf.keras.layers.Dropout(0.25)(conv)\n",
    "\n",
    "    for _ in range(num_cnn_blocks):\n",
    "        conv = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=5, padding=\"same\")(conv)\n",
    "        conv = tf.keras.layers.LeakyReLU(alpha=0.2)(conv)\n",
    "        conv = tf.keras.layers.Dropout(0.25)(conv)\n",
    "\n",
    "    #flatten the output and create a de#nse output layer\n",
    "    conv = tf.keras.layers.Flatten()(conv)\n",
    "    conv = tf.keras.layers.Dense(units=1)(conv)\n",
    "\n",
    "    return tf.keras.Model(inputs=input, outputs=conv, name=\"critic_x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Make an critic_z layer </h1>\n",
    "    - Build the critic subnetwork for the GAN. This distinguishes between the timeseries sequence and the generated sequence.\n",
    "    </br>\n",
    "    - The critic uses two fully connected layers to  distinguish between the real encoded sequence and fake encoding sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_critic_z(latent_space_dim: Tuple[int , int] ,num_dense_units: int = 100)->tf.keras.Model:\n",
    "    \"\"\"\n",
    "        Builds the critic model for critic_z\n",
    "\n",
    "        :param latent_space_dim: shaoe of the latent space\n",
    "\n",
    "        :param num_dense_units: Number of units in the dense layer\n",
    "\n",
    "        :return: Critic model\n",
    "    \"\"\"\n",
    "\n",
    "    input = tf.keras.layers.Input(shape=latent_space_dim, name=\"critic_z_input\")\n",
    "\n",
    "    dense = tf.keras.layers.Flatten()(input)\n",
    "    dense = tf.keras.layers.Dense(units=num_dense_units)(dense)\n",
    "    dense = tf.keras.layers.LeakyReLU(alpha=0.2)(dense)\n",
    "    dense = tf.keras.layers.Dropout(0.25)(dense)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(units=num_dense_units)(dense)\n",
    "    dense = tf.keras.layers.LeakyReLU(alpha=0.2)(dense)\n",
    "    dense = tf.keras.layers.Dropout(0.25)(dense)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(units=1)(dense)\n",
    "\n",
    "    return tf.keras.Model(inputs=input, outputs=dense, name=\"critic_z\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>calculate gradient penalty </h1>\n",
    "    <h2></h2>\n",
    "    - The gradient penalty is used to ensure that the critic is not too sure about the discriminator's ability to distinguish between the real and fake sequences.\n",
    "    <br>\n",
    "    - This reguralizations is used to reduce the risk of gradient exploding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def critic_x_gradient_penalty(critic_x , batch_size, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the gradient penalty.\n",
    "    \"\"\"\n",
    "    alpha = tf.keras.backend.random_uniform((batch_size, 1, 1))\n",
    "    interpolated = (alpha * y_true) + ((1 - alpha) * y_pred)\n",
    "\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        # 1. Get the discriminator output for this interpolated image.\n",
    "        pred = critic_x(interpolated)\n",
    "\n",
    "    grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "\n",
    "    return gp\n",
    "\n",
    "@tf.function\n",
    "def critic_z_gradient_penalty(critic_z, batch_size, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the gradient penalty.\n",
    "    \"\"\"\n",
    "    alpha = tf.keras.backend.random_uniform((batch_size, 1, 1))\n",
    "    interpolated = (alpha * y_true) + ((1 - alpha) * y_pred)\n",
    "\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        # 1. Get the discriminator output for this interpolated image.\n",
    "        pred = critic_z(interpolated)\n",
    "\n",
    "    # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "    grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
    "    gp = tf.reduce_mean((1.0 - norm) ** 2)\n",
    "    \n",
    "    return gp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Calculate loss function </h1>\n",
    "\n",
    "- do loss calculations for the critics , encoder and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def critic_x_loss(generator,critic_x, critic_x_loss_fn ,gradient_penelty_weight ,   x_mb, z, valid, fake, mini_batch_size):\n",
    "    \"\"\"\n",
    "    Do a step forward and calculate the loss on the critic x model\n",
    "\n",
    "    :param generator: Generator model\n",
    "    :param critic_x: Critic x model\n",
    "    :param critic_x_loss_fn: Critic x loss function\n",
    "    :param gradient_penalty_weight: Weight of the gradient penalty\n",
    "    :param x_mb: Minibatch of input data\n",
    "    :param z: Minibatch of random noise\n",
    "    :param valid: Ground truth vector for valid samples\n",
    "    :param fake: Ground truth vector for fake samples\n",
    "    :param mini_batch_size:\n",
    "\n",
    "    :return: A tuple containing the total loss and the three single losses\n",
    "    \"\"\"\n",
    "    # Do a step forward on critic x model and \n",
    "    x_ = generator(z)\n",
    "    \n",
    "    fake_x = critic_x(x_)\n",
    "    valid_x = critic_x(x_mb)\n",
    "\n",
    "    # Calculate critic x loss\n",
    "    critic_x_valid_cost = critic_x_loss_fn(y_true=valid, y_pred=valid_x)\n",
    "    critic_x_fake_cost = critic_x_loss_fn(y_true=fake, y_pred=fake_x)\n",
    "    # TODO: [SMe] Is the mini_batch size still required?\n",
    "    critic_x_grad_penalty = critic_x_gradient_penalty(critic_x  , mini_batch_size, x_mb , x_)\n",
    "    critic_x_total_loss = critic_x_valid_cost + critic_x_fake_cost + (critic_x_grad_penalty * gradient_penelty_weight)\n",
    "\n",
    "    return critic_x_total_loss, critic_x_valid_cost, critic_x_fake_cost, critic_x_grad_penalty\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def critic_z_loss(encoder, critic_z, critic_z_loss_fn, gradient_penelty_weight,x_mb, z, valid, fake, mini_batch_size):\n",
    "    \"\"\"\n",
    "    Do a step forward and calculate the loss on the critic z model\n",
    "\n",
    "    :param encoder: Encoder model\n",
    "    :param critic_z: Critic z model\n",
    "    :param critic_z_loss_fn: Critic z loss function\n",
    "    :param gradient_penalty_weight: Weight of the gradient penalty\n",
    "    :param x_mb: Minibatch of input data\n",
    "    :param z: Minibatch of random noise\n",
    "    :param valid: Ground truth vector for valid samples\n",
    "    :param fake: Ground truth vector for fake samples\n",
    "    :param mini_batch_size:\n",
    "\n",
    "    :return: A tuple containing the total loss and the three single losses\n",
    "    \"\"\"\n",
    "    # Do a step forward on critic z model and collect gradients\n",
    "    z_ = encoder(x_mb)\n",
    "    fake_z = critic_z(z_)\n",
    "    valid_z = critic_z(z) \n",
    "\n",
    "    # Calculate critic z loss\n",
    "    critic_z_valid_cost = critic_z_loss_fn(y_true=valid, y_pred=valid_z)\n",
    "    critic_z_fake_cost = critic_z_loss_fn(y_true=fake, y_pred=fake_z)\n",
    "    critic_z_grad_penalty = critic_z_gradient_penalty(critic_z , mini_batch_size, z, z_)\n",
    "    critic_z_total_loss = critic_z_valid_cost + critic_z_fake_cost + (critic_z_grad_penalty * gradient_penelty_weight)\n",
    "    return critic_z_total_loss, critic_z_valid_cost, critic_z_fake_cost, critic_z_grad_penalty\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def encoder_generator_loss(generator , encoder , critic_x , critic_z , encoder_generator_loss_fn , x_mb, z, valid):\n",
    "    \"\"\"\n",
    "    Do a step forward and calculate the loss on the encoder-generator model\n",
    "\n",
    "    :param generator: Generator model\n",
    "    :param encoder: Encoder model\n",
    "    :param critic_x: Critic x model\n",
    "    :param critic_z: Critic z model\n",
    "    :param encoder_generator_loss_fn: Encoder-generator loss function\n",
    "    :param x_mb: Minibatch of input data\n",
    "    :param z: Minibatch of random noise\n",
    "    :param valid: Ground truth vector for valid samples\n",
    "\n",
    "    :return:   Do ale containing the total loss and the three s\n",
    "    print(x_mb.shape)ingle losses\n",
    "    \"\"\"\n",
    "    # Do a step forward on the encode generator model\n",
    "    x_gen_ = generator(z)\n",
    "    fake_gen_x = critic_x(x_gen_)\n",
    "\n",
    "    z_gen_ = encoder(x_mb)\n",
    "    x_gen_rec = generator(z_gen_)\n",
    "    fake_gen_z = critic_z(z_gen_)\n",
    "\n",
    "    # Calculate encoder generator loss\n",
    "    encoder_generator_fake_gen_x_cost = encoder_generator_loss_fn(y_true=valid, y_pred=fake_gen_x)\n",
    "    encoder_generator_fake_gen_z_cost = encoder_generator_loss_fn(y_true=valid, y_pred=fake_gen_z)\n",
    "\n",
    "    # Use simple MSE as reconstruction error\n",
    "    general_reconstruction_cost = tf.reduce_mean(tf.square((x_mb - x_gen_rec)))\n",
    "    encoder_generator_total_loss = encoder_generator_fake_gen_x_cost + encoder_generator_fake_gen_z_cost + (10.0 * general_reconstruction_cost)\n",
    "    return encoder_generator_total_loss, encoder_generator_fake_gen_x_cost, encoder_generator_fake_gen_z_cost, general_reconstruction_cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training step for the model </h1>\n",
    "- do training step for the critics , encoder and generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input, n_iterations_critic,encoder, generator,critic_x , critic_z) -> dict:\n",
    "    \"\"\"\n",
    "    Custom training step for this Subclassing API Keras model.\n",
    "    The shape should be (n_iterations_critic * batch size, n_channels) because the critic networks are trained\n",
    "    multiple times over the encoder-generator network.\n",
    "\n",
    "    :param X: Group of mini batches that are used to train the critics and the encoder-generator network\n",
    "                Shape: (batch_size, signal_length, n_channels)\n",
    "    :param n_iterations_critic: Number of iterations to train the critic network\n",
    "    :param encoder : encoder model\n",
    "    :param generator : generator model\n",
    "    :param critic_x : critic_x model\n",
    "    :param critic_z : critic_z model\n",
    "    :param encoder_generator_loss: Encoder-generator loss function\n",
    "    :param critic_x_optimizer: Critic x optimizer\n",
    "    :param critic_z_optimizer: Critic z optimizer\n",
    "\n",
    "\n",
    "    :return: Sub-model losses as los dict\n",
    "    \"\"\"\n",
    "    # Get the input data\n",
    "    X = input[0] if isinstance(input , tuple) else input\n",
    "    batch_size = X.shape[0]\n",
    "    minibatch_size = batch_size//n_iterations_critic\n",
    "\n",
    "\n",
    "    #prepare ground truth data\n",
    "    valid = tf.ones((minibatch_size, 1))\n",
    "    fake = -tf.ones((minibatch_size, 1))\n",
    "\n",
    "    critic_x_loss_steps = []\n",
    "    critic_z_loss_steps = []\n",
    "\n",
    "    for critic_train_steps in range(n_iterations_critic):\n",
    "        z = tf.random.normal((minibatch_size, latent_dim, 1))\n",
    "        x_mb = X[critic_train_steps * minibatch_size:(critic_train_steps + 1) * minibatch_size]\n",
    "        #convert from (x , y ) -> (None , x , y) \n",
    "        x_mb = x_mb[None]\n",
    "        #optimize step on critic x mode                    \n",
    "        with tf.GradientTape() as tape:\n",
    "            _critic_x_losses = critic_x_loss(generator, critic_x, critic_x_loss_fn, gradient_penalty_weight , x_mb, z, valid, fake, minibatch_size)\n",
    "        \n",
    "        #backward step on critic x model\n",
    "        critic_x_gradient = tape.gradient(_critic_x_losses[0], critic_x.trainable_variables)\n",
    "        critic_x_optimizer.apply_gradients(zip(critic_x_gradient, critic_x.trainable_variables))\n",
    "\n",
    "        _critic_x_losses = np.array(_critic_x_losses)\n",
    "        critic_x_loss_steps.append(_critic_x_losses)\n",
    "\n",
    "\n",
    "        #optimize step on critic z model\n",
    "        with tf.GradientTape() as tape:\n",
    "            _critic_z_losses = critic_z_loss(encoder , critic_z , critic_z_loss_fn, gradient_penalty_weight , x_mb, z, valid, fake, minibatch_size)\n",
    "            \n",
    "        #backward step on critic z model\n",
    "        critic_z_gradient = tape.gradient(_critic_z_losses[0], critic_z.trainable_variables)\n",
    "        critic_z_optimizer.apply_gradients(zip(critic_z_gradient, critic_z.trainable_variables))\n",
    "\n",
    "        _critic_z_losses = np.array(_critic_z_losses)\n",
    "        critic_z_loss_steps.append(_critic_z_losses)\n",
    "\n",
    "    #optimize step on encoder-generator model\n",
    "    with tf.GradientTape() as tape:\n",
    "        #step forward for the generator model\n",
    "        _encoder_generator_losses = encoder_generator_loss(generator, encoder, critic_x, critic_z, encoder_generator_loss_fn, x_mb, z, valid)\n",
    "\n",
    "    #backward step on encoder-generator model\n",
    "    encoder_generator_gradient = tape.gradient(_encoder_generator_losses, encoder.trainable_variables +  generator.trainable_variables)\n",
    "    encoder_generator_optimizer.apply_gradients(zip(encoder_generator_gradient, encoder.trainable_variables + generator.trainable_variables))\n",
    "    \n",
    "    critic_x_losses = np.mean(np.array(critic_x_loss_steps), axis=0)\n",
    "    critic_z_losses = np.mean(np.array(critic_z_loss_steps), axis=0)\n",
    "    encoder_generator_losses = np.array(_encoder_generator_losses)\n",
    "\n",
    "    if log_all_losses:\n",
    "        return {\n",
    "            \"Cx_total\": critic_x_losses[0],\n",
    "            \"Cx_valid\": critic_x_losses[1],\n",
    "            \"Cx_fake\": critic_x_losses[2],\n",
    "            \"Cx_gp_penalty\": critic_x_losses[3],\n",
    "\n",
    "            \"Cz_total\": critic_z_losses[0],\n",
    "            \"Cz_valid\": critic_z_losses[1],\n",
    "            \"Cz_fake\": critic_z_losses[2],\n",
    "            \"Cz_gp_penalty\": critic_z_losses[3],\n",
    "\n",
    "            \"EG_total\": encoder_generator_losses[0],\n",
    "            \"EG_fake_gen_x\": encoder_generator_losses[1],\n",
    "            \"EG_fake_gen_z\": encoder_generator_losses[2],\n",
    "            \"G_rec\": encoder_generator_losses[3],\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"Cx_total\": critic_x_losses[0],\n",
    "            \"Cz_total\": critic_z_losses[0],\n",
    "            \"EG_total\": encoder_generator_losses[0]\n",
    "        }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(input,encoder , generator , critic_x , critic_z, gradient_penalty_weight):\n",
    "    \"\"\"\n",
    "    \n",
    "    test step for model\n",
    "    overrides model.evaluate\n",
    "\n",
    "    :param input: minibatch of the time series signals (batce_size , signal_length , n_channels)\n",
    "    :param critic_x_loss: loss function for critic x\n",
    "    :param critic_z_loss: loss function for critic z\n",
    "    :param encoder_generator_loss: loss function for encoder and generator\n",
    "    :param graident_penalty_weight: penalty weight for gradient\n",
    "\n",
    "    :return: sub-model losses as loss dict\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(input, tuple):\n",
    "            input = input[0]\n",
    "\n",
    "    batch_size = input.shape[0]\n",
    "\n",
    "    fake = tf.ones((batch_size , 1))\n",
    "    valid = tf.ones((batch_size , 1))\n",
    "\n",
    "\n",
    "    critic_x_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    critic_z_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    encoder_generator_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    encoder_generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    critic_x_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    critic_z_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    z = tf.random.normal(shape=(batch_size , latent_dim , 1))\n",
    "\n",
    "    critic_x_losses = critic_x_loss(generator,critic_x, critic_x_loss_fn ,gradient_penalty_weight , input , z, valid, fake, batch_size)\n",
    "    critic_z_losses = critic_z_loss(encoder , critic_z, critic_z_loss_fn, gradient_penalty_weight, input , z, valid, fake, batch_size)\n",
    "    encoder_generator_losses = encoder_generator_loss(generator , encoder , critic_x , critic_z , encoder_generator_loss_fn , input , z, valid)\n",
    "\n",
    "\n",
    "    if log_all_losses:\n",
    "        loss_dict = {\n",
    "            \"Cx_total\": critic_x_losses[0],\n",
    "            \"Cx_valid\": critic_x_losses[1],\n",
    "            \"Cx_fake\": critic_x_losses[2],\n",
    "            \"Cx_gp_penalty\": critic_x_losses[3],\n",
    "\n",
    "            \"Cz_total\": critic_z_losses[0],\n",
    "            \"Cz_valid\": critic_z_losses[1],\n",
    "            \"Cz_fake\": critic_z_losses[2],\n",
    "            \"Cz_gp_penalty\": critic_z_losses[3],\n",
    "\n",
    "            \"EG_total\": encoder_generator_losses[0],\n",
    "            \"EG_fake_gen_x\": encoder_generator_losses[1],\n",
    "            \"EG_fake_gen_z\": encoder_generator_losses[2],\n",
    "            \"G_rec\": encoder_generator_losses[3],\n",
    "        }\n",
    "    else:\n",
    "        loss_dict = {\n",
    "            \"Cx_total\": critic_x_losses[0],\n",
    "            \"Cz_total\": critic_z_losses[0],\n",
    "            \"EG_total\": encoder_generator_losses[0]\n",
    "        }\n",
    "\n",
    "    return loss_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def call( input, encoder , generator , critic_x , critic_z, **kwargs) -> Tuple[np.array, np.array, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Default forward step during the inference step of the model (for training and evaluation see train_step() and test_step()).\n",
    "    This function will be called in the model.predict() function to use the trained sub networks for anomaly detection.\n",
    "\n",
    "    :param X: Batch of signals that should be analyzed by the model (batch_size, signal_length, n_channels)\n",
    "\n",
    "    :param kwargs: Additional kwargs forwarded to the super class fit method\n",
    "\n",
    "    :return: Tuple containing the outputs of the sub networks as numpy arrays:\n",
    "                - The reconstructed signals from the generator\n",
    "                - The compressed embedding of the time series (latent_dim, 1) from the encoder\n",
    "                - The fake/real classification result for the reconstructed time series from the critic x network\n",
    "                - The fake/real classification result for the learned embedding from the critic z network\n",
    "    \"\"\"\n",
    "    X = input[0] if isinstance(input , tuple) else input\n",
    "    latent_encoding = encoder(X)\n",
    "    y_hat = generator(latent_encoding)\n",
    "    critic_x = critic_x(X)\n",
    "    critic_z = critic_z(latent_encoding)\n",
    "    return y_hat, latent_encoding, critic_x, critic_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Generate the models to be used during training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = generate_encoder(ts_input_shape , lstm_units=100 , latent_dim=latent_dim)\n",
    "generator = generate_generator((latent_dim , 1) , lstm_units=100)\n",
    "critic_x = build_critic_x(ts_input_shape)\n",
    "critic_z = build_critic_z((latent_dim , 1)) \n",
    "\n",
    "print(encoder.summary())\n",
    "print(generator.summary())\n",
    "print(critic_x.summary())\n",
    "print(critic_z.summary())\n",
    "\n",
    "encoder_checkpoint = \"training_checkpoints/encoder-{epoch:04d}.cpkt\"\n",
    "generator_checkpoint = \"training_checkpoints/generator-{epoch:04d}.cpkt\"\n",
    "critic_x_checkpoint = \"training_checkpoints/ciritic_x-{epoch:04d}.cpkt\"\n",
    "critic_z_checkpoint = \"training_checkpoints/critic_z-{epoch:04d}.cpkt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tadGan = TadGAN()\n",
    "print(tadGan.encoder.summary())\n",
    "print(tadGan.generator.summary())\n",
    "print(tadGan.critic_x.summary())\n",
    "print(tadGan.critic_z.summary())\n",
    "tadGan.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Get the Dataset</h1>\n",
    "- create a tf batch dataset out of the CSV\n",
    "<br>\n",
    "- data should not be shuffeled as the data's sequential information needs to be preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.data.experimental.enable_debug_mode()\n",
    "def pack_features_vector(features, labels):\n",
    "  \"\"\"Pack the features into a single array.\"\"\"\n",
    "  features = tf.stack(list(features.values()), axis=1)\n",
    "  return features, labels\n",
    "\n",
    "#np_dataset = pd.read_csv('fan_speed_vibration_without_duration.csv').to_numpy()\n",
    "#print(np_dataset.shape)\n",
    "dataset = tf.data.experimental.make_csv_dataset(\"fan_speed_vibration_without_duration.csv\", batch_size = seq_len * n_iterations_critic ,shuffle=False)\n",
    "#dataset = tf.data.Dataset.from_tensor_slices(np_dataset)\n",
    "#dataset = dataset.batch(seq_len * n_iterations_critic)\n",
    "\n",
    "#dataset = dataset.shuffle(buffer_size=dataset.size[0])\n",
    "\n",
    "#dataset = dataset.shuffle(buffer_size=1024).batch(seq_len)\n",
    "\n",
    "dataset\n",
    "#tf.data.experimental.CsvDataset(\"fan_speed_vibration_without_duration.csv\", [tf.float32 , tf.float32 , tf.float32 . tf.float32, tf.float32 , tf.float32 , tf.float32 . tf.float32])\n",
    "\n",
    "#dataset = pd.read_csv(\"fan_speed_vibration_without_duration.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training the model</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(False)\n",
    "epoch = 20\n",
    "save_dir = \"models/\"\n",
    "with trange(epoch , position=0 , unit=\"epoch\") as pbar:\n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch {epoch}\")\n",
    "            for step , data in enumerate(dataset):\n",
    "                data = list(data.values())\n",
    "                data = np.array(data)\n",
    "                old_shape = data.shape\n",
    "                data = data.reshape(old_shape[1] , old_shape[0])\n",
    "                loss_dict = train_step(data, n_iterations_critic , encoder , generator , critic_x , critic_z)\n",
    "                pbar.set_postfix(loss_dict , refresh=True)\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                encoder.save_weights(encoder_checkpoint.format(epoch))\n",
    "                generator.save_weights(generator_checkpoint.format(epoch))\n",
    "                critic_x.save_weights(critic_x_checkpoint.format(epoch))\n",
    "                critic_z.save_weights(critic_z_checkpoint.format(epoch))\n",
    "                \n",
    "        \n",
    "        encoder.save(\"models/encoder\")\n",
    "        generator.save(\"models/generator\")\n",
    "        critic_x.save(\"models/critic_x\")\n",
    "        critic_z.save(\"model/critic_z\")\n",
    "'''\n",
    "tadGan = TadGAN()\n",
    "print(tadGan.encoder.summary())\n",
    "print(tadGan.generator.summary())\n",
    "print(tadGan.critic_x.summary())\n",
    "print(tadGan.critic_z.summary())\n",
    "tadGan.summary()\n",
    "'''\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1b7e350b82617662beaaacd567468af7b9ee2240126ae9c7aedafcf99e0b4bb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
