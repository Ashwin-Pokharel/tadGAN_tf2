{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras from tensorflow\n",
    "import os\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tadgan_example import TadGAN\n",
    "from tqdm import trange\n",
    "import sys\n",
    "import tensorflow.python.ops.numpy_ops.np_config as np_config\n",
    "np_config.enable_numpy_behavior()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>function for wasserstein loss</h1>\n",
    "- described in the [Wasserstein GAN](https://arxiv.org/abs/1701.07875)\n",
    "<br>\n",
    "- implemented in [tadGAN](https://arxiv.org/pdf/2009.07769.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _wasserstein_loss(y_true, y_pred):\n",
    "    return tf.keras.backend.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Define all of the input shapes and parameters</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 100 # sequence length must always be even\n",
    "ts_input_shape: Tuple[int] = (seq_len, 8)#this is for univariant data , change the shape accorindly ex for multivariant data change to (100,num_features)\n",
    "latent_dim: int = 20 #latent dimension where encoder and decoder will be trained\n",
    "gradient_penalty_weight: int = 10#gradient penelty weight for wasserstein loss\n",
    "n_iterations_critic: int = 5#number of iterations for training the critic per iter for encoder and decoder\n",
    "\n",
    "# sub network hyper parameters\n",
    "encoder_lstm_units: int = 100 # number of units in encoder LSTM\n",
    "generator_lstm_units: int = 100 # number of units in generator LSTM\n",
    "generator_output_activation: str = \"tanh\" # activation function for generator output\n",
    "critic_x_cnn_blocks: int = 4 # number of convolutional blocks in critic x\n",
    "critic_x_cnn_filters: int = 64 # number of filters in each convolutional block in critic x\n",
    "critic_z_dense_units: int = 100 # number of units in critic z dense layer\n",
    "\n",
    "log_all_losses: bool = True\n",
    "print_model_summaries: bool = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Make an encoder layer </h1>\n",
    "    - Build the Encoder subnetwork for the GAN. This model learns the compressed representation of the input and transforms the timesries sequence into the latent space\n",
    "    </br>\n",
    "    - The encoder uses a single layer BI-LSTM network to learn the compressed representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_encoder(input_shape: Tuple[int]=(100,1), lstm_units:int = 100, latent_dim:int=20)->tf.keras.Model:\n",
    "    \"\"\"\n",
    "        The number of LSTM units can be adjusted.\n",
    "\n",
    "        :param lstm_units: Number of LSTM units that could be used for the time series encoding\n",
    "\n",
    "        :input_shape: Tuple of input shape (batch_size, len_sequence, num_features)\n",
    "\n",
    "        :return: Encoder model\n",
    "    \"\"\"\n",
    "\n",
    "    input = tf.keras.layers.Input(shape=input_shape , name=\"encoder_input\")\n",
    "    #create a bi-directional LSTM layer\n",
    "    encoded = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=lstm_units, return_sequences=True))(input)\n",
    "    encoded = tf.keras.layers.Flatten()(encoded)\n",
    "    encoded = tf.keras.layers.Dense(units=latent_dim, name=\"latent_encoding\")(encoded)\n",
    "    encoded = tf.keras.layers.Reshape(target_shape=(latent_dim, 1) , name=\"output_encoder\")(encoded)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=encoded, name=\"encoder\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Make an generator layer </h1>\n",
    "    - Build the generator subnetwork for the GAN. This recreates the timeseries sequence from the latent space\n",
    "    </br>\n",
    "    - The generator  uses a double  layer BI-LSTM network to recreate the timeseries data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_generator(latent_shape: Tuple[int], lstm_units:int = 64, activation_function:str=\"tanh\") -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "        The number of LSTM units can be adjusted.\n",
    "\n",
    "        :param lstm_units: Number of LSTM units that could be used for the time series generation\n",
    "\n",
    "        :param latent_shape: Shape of the latent encoding\n",
    "\n",
    "        :param activation_function: final activation layer for the generator \n",
    "\n",
    "        :return: Generator model\n",
    "    \"\"\"\n",
    "\n",
    "    input = tf.keras.layers.Input(shape=latent_shape, name=\"generator_input\")\n",
    "    decoded = tf.keras.layers.Flatten()(input)\n",
    "\n",
    "    #first layer should be half the size of the sequence\n",
    "    half_seq_length = seq_len // 2\n",
    "    decoded = tf.keras.layers.Dense(units=half_seq_length)(decoded)\n",
    "    decoded = tf.keras.layers.Reshape(target_shape=(half_seq_length, 1))(decoded)  \n",
    "\n",
    "    # generate a new timeseries using two ltsm layers that have 64 hidden units with upsampling  between them\n",
    "    decoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True, dropout=0.2 , recurrent_dropout=0.2), merge_mode=\"concat\")(decoded)\n",
    "    decoder = tf.keras.layers.UpSampling1D(size=2)(decoder)\n",
    "    decoder = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True, dropout=0.2 , recurrent_dropout=0.2), merge_mode=\"concat\")(decoder)\n",
    "\n",
    "    #rebuild the original shape of the time series for all signals\n",
    "    decoder = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(ts_input_shape[1]))(decoder)\n",
    "    decoder = tf.keras.layers.Activation(activation_function)(decoder)\n",
    "    return tf.keras.Model(inputs=input, outputs=decoder, name=\"generator\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Make an critic_x  layer </h1>\n",
    "    - Build the critic subnetwork for the GAN. This distinguishes between the timeseries sequence and the generated sequence.\n",
    "    </br>\n",
    "    - The critic uses sequence of 1d convolutional layers to distinguish between the timeseries and generated sequence. and finally a fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_critic_x(input_shape ,num_filters: int = 64, num_cnn_blocks: int = 4) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "        Builds the critic model for the critic_x\n",
    "\n",
    "        :param num_filters: Number of filters in each convolutional block\n",
    "\n",
    "        :param num_cnn_blocks: Number of convolutional blocks in the critic\n",
    "\n",
    "        :return: Critic model\n",
    "    print(input_shape)\n",
    "    \"\"\"\n",
    "    input = tf.keras.layers.Input(shape=input_shape, name=\"critic_x_input\")\n",
    "    #create a convolutional layer with num_filters filters\n",
    "    conv = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=5)(input)\n",
    "    conv = tf.keras.layers.LeakyReLU(alpha=0.2)(conv)\n",
    "    conv = tf.keras.layers.Dropout(0.25)(conv)\n",
    "\n",
    "    for _ in range(num_cnn_blocks):\n",
    "        conv = tf.keras.layers.Conv1D(filters=num_filters, kernel_size=5, padding=\"same\")(conv)\n",
    "        conv = tf.keras.layers.LeakyReLU(alpha=0.2)(conv)\n",
    "        conv = tf.keras.layers.Dropout(0.25)(conv)\n",
    "\n",
    "    #flatten the output and create a de#nse output layer\n",
    "    conv = tf.keras.layers.Flatten()(conv)\n",
    "    conv = tf.keras.layers.Dense(units=1)(conv)\n",
    "\n",
    "    return tf.keras.Model(inputs=input, outputs=conv, name=\"critic_x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Make an critic_z layer </h1>\n",
    "    - Build the critic subnetwork for the GAN. This distinguishes between the timeseries sequence and the generated sequence.\n",
    "    </br>\n",
    "    - The critic uses two fully connected layers to  distinguish between the real encoded sequence and fake encoding sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_critic_z(latent_space_dim: Tuple[int , int] ,num_dense_units: int = 100)->tf.keras.Model:\n",
    "    \"\"\"\n",
    "        Builds the critic model for critic_z\n",
    "\n",
    "        :param latent_space_dim: shaoe of the latent space\n",
    "\n",
    "        :param num_dense_units: Number of units in the dense layer\n",
    "\n",
    "        :return: Critic model\n",
    "    \"\"\"\n",
    "\n",
    "    input = tf.keras.layers.Input(shape=latent_space_dim, name=\"critic_z_input\")\n",
    "\n",
    "    dense = tf.keras.layers.Flatten()(input)\n",
    "    dense = tf.keras.layers.Dense(units=num_dense_units)(input)\n",
    "    dense = tf.keras.layers.LeakyReLU(alpha=0.2)(dense)\n",
    "    dense = tf.keras.layers.Dropout(0.25)(dense)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(units=num_dense_units)(dense)\n",
    "    dense = tf.keras.layers.LeakyReLU(alpha=0.2)(dense)\n",
    "    dense = tf.keras.layers.Dropout(0.25)(dense)\n",
    "    \n",
    "    dense = tf.keras.layers.Dense(units=1)(dense)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input, outputs=dense, name=\"critic_z\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>calculate gradient penalty </h1>\n",
    "    <h2></h2>\n",
    "    - The gradient penalty is used to ensure that the critic is not too sure about the discriminator's ability to distinguish between the real and fake sequences.\n",
    "    <br>\n",
    "    - This reguralizations is used to reduce the risk of gradient exploding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def critic_x_gradient_penalty(critic_x , batch_size, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the gradient penalty.\n",
    "    \"\"\"\n",
    "    alpha = tf.keras.backend.random_uniform((batch_size, 1, 1))\n",
    "    interpolated = (alpha * y_true) + ((1 - alpha) * y_pred)\n",
    "\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        # 1. Get the discriminator output for this interpolated image.\n",
    "        pred = critic_x(interpolated)\n",
    "\n",
    "    grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
    "    gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "\n",
    "    return gp\n",
    "\n",
    "@tf.function\n",
    "def critic_z_gradient_penalty(critic_z, batch_size, y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the gradient penalty.\n",
    "    \"\"\"\n",
    "    alpha = tf.keras.backend.random_uniform((batch_size, 1, 1))\n",
    "    interpolated = (alpha * y_true) + ((1 - alpha) * y_pred)\n",
    "\n",
    "    with tf.GradientTape() as gp_tape:\n",
    "        gp_tape.watch(interpolated)\n",
    "        # 1. Get the discriminator output for this interpolated image.\n",
    "        pred = critic_z(interpolated)\n",
    "\n",
    "    # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "    grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
    "    gp = tf.reduce_mean((1.0 - norm) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Calculate loss function </h1>\n",
    "\n",
    "- do loss calculations for the critics , encoder and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def critic_x_loss(generator,critic_x, critic_x_loss_fn ,gradient_penelty_weight ,   x_mb, z, valid, fake, mini_batch_size):\n",
    "    \"\"\"\n",
    "    Do a step forward and calculate the loss on the critic x model\n",
    "\n",
    "    :param generator: Generator model\n",
    "    :param critic_x: Critic x model\n",
    "    :param critic_x_loss_fn: Critic x loss function\n",
    "    :param gradient_penalty_weight: Weight of the gradient penalty\n",
    "    :param x_mb: Minibatch of input data\n",
    "    :param z: Minibatch of random noise\n",
    "    :param valid: Ground truth vector for valid samples\n",
    "    :param fake: Ground truth vector for fake samples\n",
    "    :param mini_batch_size:\n",
    "\n",
    "    :return: A tuple containing the total loss and the three single losses\n",
    "    \"\"\"\n",
    "    # Do a step forward on critic x model and \n",
    "    x_ = generator(z)\n",
    "    \n",
    "    fake_x = critic_x(x_)\n",
    "    valid_x = critic_x(x_mb)\n",
    "\n",
    "    # Calculate critic x loss\n",
    "    critic_x_valid_cost = critic_x_loss_fn(y_true=valid, y_pred=valid_x)\n",
    "    critic_x_fake_cost = critic_x_loss_fn(y_true=fake, y_pred=fake_x)\n",
    "    # TODO: [SMe] Is the mini_batch size still required?\n",
    "    critic_x_grad_penalty = critic_x_gradient_penalty(mini_batch_size, x_mb, x_)\n",
    "    critic_x_total_loss = critic_x_valid_cost + critic_x_fake_cost + (critic_x_grad_penalty * gradient_penelty_weight)\n",
    "\n",
    "    return critic_x_total_loss, critic_x_valid_cost, critic_x_fake_cost, critic_x_grad_penalty\n",
    "\n",
    "@tf.function\n",
    "def critic_z_loss(encoder , critic_z, critic_z_loss_fn, gradient_penelty_weight,x_mb, z, valid, fake, mini_batch_size):\n",
    "    \"\"\"\n",
    "    Do a step forward and calculate the loss on the critic z model\n",
    "\n",
    "    :param encoder: Encoder model\n",
    "    :param critic_z: Critic z model\n",
    "    :param critic_z_loss_fn: Critic z loss function\n",
    "    :param gradient_penalty_weight: Weight of the gradient penalty\n",
    "    :param x_mb: Minibatch of input data\n",
    "    :param z: Minibatch of random noise\n",
    "    :param valid: Ground truth vector for valid samples\n",
    "    :param fake: Ground truth vector for fake samples\n",
    "    :param mini_batch_size:\n",
    "\n",
    "    :return: A tuple containing the total loss and the three single losses\n",
    "    \"\"\"\n",
    "    # Do a step forward on critic z model and collect gradients\n",
    "    z_ = encoder(x_mb)\n",
    "    fake_z = critic_z(z_)\n",
    "    valid_z = critic_z(z)\n",
    "\n",
    "    # Calculate critic z loss\n",
    "    critic_z_valid_cost = critic_z_loss_fn(y_true=valid, y_pred=valid_z)\n",
    "    critic_z_fake_cost = critic_z_loss_fn(y_true=fake, y_pred=fake_z)\n",
    "    critic_z_gradient_penalty = critic_z_gradient_penalty(mini_batch_size, z, z_)\n",
    "    critic_z_total_loss = critic_z_valid_cost + critic_z_fake_cost + (critic_z_gradient_penalty * gradient_penelty_weight)\n",
    "    return critic_z_total_loss, critic_z_valid_cost, critic_z_fake_cost, critic_z_gradient_penalty\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def encoder_generator_loss(generator , encoder , critic_x , critic_z , encoder_generator_loss_fn , x_mb, z, valid):\n",
    "    \"\"\"\n",
    "    Do a step forward and calculate the loss on the encoder-generator model\n",
    "\n",
    "    :param generator: Generator model\n",
    "    :param encoder: Encoder model\n",
    "    :param critic_x: Critic x model\n",
    "    :param critic_z: Critic z model\n",
    "    :param encoder_generator_loss_fn: Encoder-generator loss function\n",
    "    :param x_mb: Minibatch of input data\n",
    "    :param z: Minibatch of random noise\n",
    "    :param valid: Ground truth vector for valid samples\n",
    "\n",
    "    :return:   Do ale containing the total loss and the three s\n",
    "    print(x_mb.shape)ingle losses\n",
    "    \"\"\"\n",
    "    # Do a step forward on the encode generator model\n",
    "    x_gen_ = generator(z)\n",
    "    fake_gen_x = critic_x(x_gen_)\n",
    "\n",
    "    z_gen_ = encoder(x_mb)\n",
    "    x_gen_rec = generator(z_gen_)\n",
    "    fake_gen_z = critic_z(z_gen_)\n",
    "\n",
    "    # Calculate encoder generator loss\n",
    "    encoder_generator_fake_gen_x_cost = encoder_generator_loss_fn(y_true=valid, y_pred=fake_gen_x)\n",
    "    encoder_generator_fake_gen_z_cost = encoder_generator_loss_fn(y_true=valid, y_pred=fake_gen_z)\n",
    "\n",
    "    # Use simple MSE as reconstruction error\n",
    "    general_reconstruction_cost = tf.reduce_mean(tf.square((x_mb - x_gen_rec)))\n",
    "    encoder_generator_total_loss = encoder_generator_fake_gen_x_cost + encoder_generator_fake_gen_z_cost + (10.0 * general_reconstruction_cost)\n",
    "\n",
    "    return encoder_generator_total_loss, encoder_generator_fake_gen_x_cost, encoder_generator_fake_gen_z_cost, general_reconstruction_cost\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training step for the model </h1>\n",
    "- do training step for the critics , encoder and generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input, n_iterations_critic,encoder, generator,critic_x , critic_z) -> dict:\n",
    "    \"\"\"\n",
    "    Custom training step for this Subclassing API Keras model.\n",
    "    The shape should be (n_iterations_critic * batch size, n_channels) because the critic networks are trained\n",
    "    multiple times over the encoder-generator network.\n",
    "\n",
    "    :param X: Group of mini batches that are used to train the critics and the encoder-generator network\n",
    "                Shape: (batch_size, signal_length, n_channels)\n",
    "    :param n_iterations_critic: Number of iterations to train the critic network\n",
    "    :param critic_x_loss: Critic x loss function\n",
    "    :param critic_z_loss: Critic z loss function\n",
    "    :param encoder_generator_loss: Encoder-generator loss function\n",
    "    :param critic_x_optimizer: Critic x optimizer\n",
    "    :param critic_z_optimizer: Critic z optimizer\n",
    "\n",
    "\n",
    "    :return: Sub-model losses as los dict\n",
    "    \"\"\"\n",
    "    # Get the input data\n",
    "    X = input[0] if isinstance(input , tuple) else input\n",
    "    batch_size = X.shape[0]\n",
    "    minibatch_size = batch_size//n_iterations_critic\n",
    "    critic_x_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    critic_z_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    encoder_generator_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    encoder_generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    critic_x_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    critic_z_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "\n",
    "    #prepare ground truth data\n",
    "    valid = tf.ones((minibatch_size, 1))\n",
    "    fake = -tf.ones((minibatch_size, 1))\n",
    "\n",
    "    critic_x_loss_steps = []\n",
    "    critic_z_loss_steps = []\n",
    "\n",
    "    for critic_train_steps in range(n_iterations_critic):\n",
    "        z = tf.random.normal((minibatch_size, latent_dim, 1))\n",
    "        x_mb = X[critic_train_steps * minibatch_size:(critic_train_steps + 1) * minibatch_size]\n",
    "        x_mb = x_mb[None]\n",
    "        #optimize step on critic x mode                    \n",
    "        with tf.GradientTape() as tape:\n",
    "            _critic_x_losses = critic_x_loss(generator, critic_x, critic_x_loss_fn, gradient_penalty_weight , x_mb, z, valid, fake, minibatch_size)\n",
    "\n",
    "        #backward step on critic x model\n",
    "        critic_x_gradient = tape.gradient(_critic_x_losses[0], critic_x.trainable_variables)\n",
    "        critic_x_optimizer.apply_gradients(zip(critic_x_gradient, critic_x.trainable_variables))\n",
    "\n",
    "        _critic_x_losses = np.array(_critic_x_losses)\n",
    "        critic_x_loss_steps.append(_critic_x_losses)\n",
    "\n",
    "\n",
    "        #optimize step on critic z model\n",
    "        with tf.GradientTape() as tape:\n",
    "            _critic_z_losses = critic_z_loss(encoder , critic_z , critic_z_loss_fn, gradient_penalty_weight , x_mb, z, valid, fake, minibatch_size)\n",
    "\n",
    "        #backward step on critic z model\n",
    "        critic_z_gradient = tape.gradient(_critic_z_losses[0], critic_z.trainable_variables)\n",
    "        critic_z_optimizer.apply_gradients(zip(critic_z_gradient, critic_z.trainable_variables))\n",
    "\n",
    "        _critic_z_losses = np.array(_critic_z_losses)\n",
    "        critic_z_loss_steps.append(_critic_z_losses)\n",
    "\n",
    "    #optimize step on encoder-generator model\n",
    "    with tf.GradientTape() as tape:\n",
    "        #step forward for the generator model\n",
    "        _encoder_generator_losses = encoder_generator_loss(generator, encoder, critic_x, critic_z, encoder_generator_loss_fn, X, z, valid)\n",
    "\n",
    "    #backward step on encoder-generator model\n",
    "    encoder_generator_gradient = tape.gradient(_encoder_generator_losses, encoder.trainable_variables +  generator.trainable_variables)\n",
    "    encoder_generator_optimizer.apply_gradients(zip(encoder_generator_gradient, encoder.trainable_variables + generator.trainable_variables))\n",
    "\n",
    "\n",
    "    critic_x_losses = np.mean(np.array(critic_x_loss_steps), axis=0)\n",
    "    critic_z_losses = np.mean(np.array(critic_z_loss_steps), axis=0)\n",
    "    encoder_generator_losses = np.array(_encoder_generator_losses)\n",
    "\n",
    "    if log_all_losses:\n",
    "        return {\n",
    "            \"Cx_total\": critic_x_losses[0],\n",
    "            \"Cx_valid\": critic_x_losses[1],\n",
    "            \"Cx_fake\": critic_x_losses[2],\n",
    "            \"Cx_gp_penalty\": critic_x_losses[3],\n",
    "\n",
    "            \"Cz_total\": critic_z_losses[0],\n",
    "            \"Cz_valid\": critic_z_losses[1],\n",
    "            \"Cz_fake\": critic_z_losses[2],\n",
    "            \"Cz_gp_penalty\": critic_z_losses[3],\n",
    "\n",
    "            \"EG_total\": encoder_generator_losses[0],\n",
    "            \"EG_fake_gen_x\": encoder_generator_losses[1],\n",
    "            \"EG_fake_gen_z\": encoder_generator_losses[2],\n",
    "            \"G_rec\": encoder_generator_losses[3],\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"Cx_total\": critic_x_losses[0],\n",
    "            \"Cz_total\": critic_z_losses[0],\n",
    "            \"EG_total\": encoder_generator_losses[0]\n",
    "        }\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(input,encoder , generator , critic_x , critic_z, gradient_penalty_weight):\n",
    "    \"\"\"\n",
    "    \n",
    "    test step for model\n",
    "    overrides model.evaluate\n",
    "\n",
    "    :param input: minibatch of the time series signals (batce_size , signal_length , n_channels)\n",
    "    :param critic_x_loss: loss function for critic x\n",
    "    :param critic_z_loss: loss function for critic z\n",
    "    :param encoder_generator_loss: loss function for encoder and generator\n",
    "    :param graident_penalty_weight: penalty weight for gradient\n",
    "\n",
    "    :return: sub-model losses as loss dict\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(input, tuple):\n",
    "            input = input[0]\n",
    "\n",
    "    batch_size = input.shape[0]\n",
    "\n",
    "    fake = tf.ones((batch_size , 1))\n",
    "    valid = tf.ones((batch_size , 1))\n",
    "\n",
    "\n",
    "    critic_x_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    critic_z_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    encoder_generator_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "    encoder_generator_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    critic_x_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    critic_z_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    z = tf.random.normal(shape=(batch_size , latent_dim , 1))\n",
    "\n",
    "    critic_x_losses = critic_x_loss(generator,critic_x, critic_x_loss_fn ,gradient_penalty_weight , input , z, valid, fake, batch_size)\n",
    "    critic_z_losses = critic_z_loss(encoder , critic_z, critic_z_loss_fn, gradient_penalty_weight, input , z, valid, fake, batch_size)\n",
    "    encoder_generator_losses = encoder_generator_loss(generator , encoder , critic_x , critic_z , encoder_generator_loss_fn , input , z, valid)\n",
    "\n",
    "\n",
    "    if log_all_losses:\n",
    "        loss_dict = {\n",
    "            \"Cx_total\": critic_x_losses[0],\n",
    "            \"Cx_valid\": critic_x_losses[1],\n",
    "            \"Cx_fake\": critic_x_losses[2],\n",
    "            \"Cx_gp_penalty\": critic_x_losses[3],\n",
    "\n",
    "            \"Cz_total\": critic_z_losses[0],\n",
    "            \"Cz_valid\": critic_z_losses[1],\n",
    "            \"Cz_fake\": critic_z_losses[2],\n",
    "            \"Cz_gp_penalty\": critic_z_losses[3],\n",
    "\n",
    "            \"EG_total\": encoder_generator_losses[0],\n",
    "            \"EG_fake_gen_x\": encoder_generator_losses[1],\n",
    "            \"EG_fake_gen_z\": encoder_generator_losses[2],\n",
    "            \"G_rec\": encoder_generator_losses[3],\n",
    "        }\n",
    "    else:\n",
    "        loss_dict = {\n",
    "            \"Cx_total\": critic_x_losses[0],\n",
    "            \"Cz_total\": critic_z_losses[0],\n",
    "            \"EG_total\": encoder_generator_losses[0]\n",
    "        }\n",
    "\n",
    "    return loss_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def call( input, encoder , generator , critic_x , critic_z, **kwargs) -> Tuple[np.array, np.array, np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Default forward step during the inference step of the model (for training and evaluation see train_step() and test_step()).\n",
    "    This function will be called in the model.predict() function to use the trained sub networks for anomaly detection.\n",
    "\n",
    "    :param X: Batch of signals that should be analyzed by the model (batch_size, signal_length, n_channels)\n",
    "\n",
    "    :param kwargs: Additional kwargs forwarded to the super class fit method\n",
    "\n",
    "    :return: Tuple containing the outputs of the sub networks as numpy arrays:\n",
    "                - The reconstructed signals from the generator\n",
    "                - The compressed embedding of the time series (latent_dim, 1) from the encoder\n",
    "                - The fake/real classification result for the reconstructed time series from the critic x network\n",
    "                - The fake/real classification result for the learned embedding from the critic z network\n",
    "    \"\"\"\n",
    "    X = input[0] if isinstance(input , tuple) else input\n",
    "    latent_encoding = encoder(X)\n",
    "    y_hat = generator(latent_encoding)\n",
    "    critic_x = critic_x(X)\n",
    "    critic_z = critic_z(latent_encoding)\n",
    "    return y_hat, latent_encoding, critic_x, critic_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Generate the models to be used during training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 100, 8)]          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_30 (Bidirectio (None, 100, 200)          87200     \n",
      "_________________________________________________________________\n",
      "flatten_40 (Flatten)         (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "latent_encoding (Dense)      (None, 20)                400020    \n",
      "_________________________________________________________________\n",
      "output_encoder (Reshape)     (None, 20, 1)             0         \n",
      "=================================================================\n",
      "Total params: 487,220\n",
      "Trainable params: 487,220\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) [(None, 20, 1)]           0         \n",
      "_________________________________________________________________\n",
      "flatten_41 (Flatten)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 50)                1050      \n",
      "_________________________________________________________________\n",
      "reshape_10 (Reshape)         (None, 50, 1)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional_31 (Bidirectio (None, 50, 200)           81600     \n",
      "_________________________________________________________________\n",
      "up_sampling1d_10 (UpSampling (None, 100, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_32 (Bidirectio (None, 100, 200)          240800    \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 100, 8)            1608      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 100, 8)            0         \n",
      "=================================================================\n",
      "Total params: 325,058\n",
      "Trainable params: 325,058\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"critic_x\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "critic_x_input (InputLayer)  [(None, 100, 8)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d_45 (Conv1D)           (None, 96, 64)            2624      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)   (None, 96, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 96, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_46 (Conv1D)           (None, 96, 64)            20544     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)   (None, 96, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 96, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_47 (Conv1D)           (None, 96, 64)            20544     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)   (None, 96, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 96, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_48 (Conv1D)           (None, 96, 64)            20544     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)   (None, 96, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_68 (Dropout)         (None, 96, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 96, 64)            20544     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)   (None, 96, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_69 (Dropout)         (None, 96, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_42 (Flatten)         (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 6145      \n",
      "=================================================================\n",
      "Total params: 90,945\n",
      "Trainable params: 90,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"critic_z\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "critic_z_input (InputLayer)  [(None, 20, 1)]           0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 20, 100)           200       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)   (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "dropout_70 (Dropout)         (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 20, 100)           10100     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)   (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "dropout_71 (Dropout)         (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 20, 1)             101       \n",
      "=================================================================\n",
      "Total params: 10,401\n",
      "Trainable params: 10,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "encoder = generate_encoder(ts_input_shape , lstm_units=100 , latent_dim=latent_dim)\n",
    "generator = generate_generator((latent_dim , 1) , lstm_units=100)\n",
    "critic_x = build_critic_x(ts_input_shape)\n",
    "critic_z = build_critic_z((latent_dim , 1)) \n",
    "\n",
    "print(encoder.summary())\n",
    "print(generator.summary())\n",
    "print(critic_x.summary())\n",
    "print(critic_z.summary())\n",
    "\n",
    "encoder_checkpoint = \"training_checkpoints/encoder-{epoch:04d}.cpkt\"\n",
    "generator_checkpoint = \"training_checkpoints/generator-{epoch:04d}.cpkt\"\n",
    "critic_x_checkpoint = \"training_checkpoints/ciritic_x-{epoch:04d}.cpkt\"\n",
    "critic_z_checkpoint = \"training_checkpoints/critic_z-{epoch:04d}.cpkt\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   [(None, 100, 1)]          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_33 (Bidirectio (None, 100, 200)          81600     \n",
      "_________________________________________________________________\n",
      "flatten_44 (Flatten)         (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "latent_encoding (Dense)      (None, 20)                400020    \n",
      "_________________________________________________________________\n",
      "output_encoder (Reshape)     (None, 20, 1)             0         \n",
      "=================================================================\n",
      "Total params: 481,620\n",
      "Trainable params: 481,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"generator_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "generator_input (InputLayer) [(None, 20, 1)]           0         \n",
      "_________________________________________________________________\n",
      "flatten_45 (Flatten)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 50)                1050      \n",
      "_________________________________________________________________\n",
      "reshape_11 (Reshape)         (None, 50, 1)             0         \n",
      "_________________________________________________________________\n",
      "bidirectional_34 (Bidirectio (None, 50, 200)           81600     \n",
      "_________________________________________________________________\n",
      "up_sampling1d_11 (UpSampling (None, 100, 200)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_35 (Bidirectio (None, 100, 200)          240800    \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 100, 1)            201       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 100, 1)            0         \n",
      "=================================================================\n",
      "Total params: 323,651\n",
      "Trainable params: 323,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"critic_x_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "critic_x_input (InputLayer)  [(None, 100, 1)]          0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 96, 64)            384       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)   (None, 96, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_72 (Dropout)         (None, 96, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 92, 64)            20544     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)   (None, 92, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 92, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 88, 64)            20544     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_74 (LeakyReLU)   (None, 88, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 88, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 84, 64)            20544     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_75 (LeakyReLU)   (None, 84, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 84, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_46 (Flatten)         (None, 5376)              0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1)                 5377      \n",
      "=================================================================\n",
      "Total params: 67,393\n",
      "Trainable params: 67,393\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"critic_z_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "critic_z_input (InputLayer)  [(None, 20, 1)]           0         \n",
      "_________________________________________________________________\n",
      "flatten_47 (Flatten)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 100)               2100      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_76 (LeakyReLU)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_76 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_77 (LeakyReLU)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_77 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 12,301\n",
      "Trainable params: 12,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Model: \"tad_gan_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_model (Functional)   (None, 20, 1)             481620    \n",
      "_________________________________________________________________\n",
      "generator_model (Functional) (None, 100, 1)            323651    \n",
      "_________________________________________________________________\n",
      "critic_x_model (Functional)  (None, 1)                 67393     \n",
      "_________________________________________________________________\n",
      "critic_z_model (Functional)  (None, 1)                 12301     \n",
      "=================================================================\n",
      "Total params: 884,965\n",
      "Trainable params: 884,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tadGan = TadGAN()\n",
    "print(tadGan.encoder.summary())\n",
    "print(tadGan.generator.summary())\n",
    "print(tadGan.critic_x.summary())\n",
    "print(tadGan.critic_z.summary())\n",
    "tadGan.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Get the Dataset</h1>\n",
    "- create a tf batch dataset out of the CSV\n",
    "<br>\n",
    "- data should not be shuffeled as the data's sequential information needs to be preserved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: OrderedDict([(signal-1, (500,)), (signal-2, (500,)), (signal-3, (500,)), (signal-4, (500,)), (signal-5, (500,)), (signal-6, (500,)), (signal-7, (500,)), (signal-8, (500,))]), types: OrderedDict([(signal-1, tf.float32), (signal-2, tf.float32), (signal-3, tf.float32), (signal-4, tf.float32), (signal-5, tf.float32), (signal-6, tf.float32), (signal-7, tf.float32), (signal-8, tf.float32)])>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pack_features_vector(features, labels):\n",
    "  \"\"\"Pack the features into a single array.\"\"\"\n",
    "  features = tf.stack(list(features.values()), axis=1)\n",
    "  return features, labels\n",
    "\n",
    "#np_dataset = pd.read_csv('fan_speed_vibration_without_duration.csv').to_numpy()\n",
    "#print(np_dataset.shape)\n",
    "dataset = tf.data.experimental.make_csv_dataset(\"fan_speed_vibration_without_duration.csv\", batch_size = seq_len * n_iterations_critic ,shuffle=False)\n",
    "#dataset = tf.data.Dataset.from_tensor_slices(np_dataset)\n",
    "#dataset = dataset.batch(seq_len * n_iterations_critic)\n",
    "\n",
    "#dataset = dataset.shuffle(buffer_size=dataset.size[0])\n",
    "\n",
    "#dataset = dataset.shuffle(buffer_size=1024).batch(seq_len)\n",
    "\n",
    "dataset\n",
    "#tf.data.experimental.CsvDataset(\"fan_speed_vibration_without_duration.csv\", [tf.float32 , tf.float32 , tf.float32 . tf.float32, tf.float32 , tf.float32 , tf.float32 . tf.float32])\n",
    "\n",
    "#dataset = pd.read_csv(\"fan_speed_vibration_without_duration.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training the model</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                   | 0/20 [00:00<?, ?epoch/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    /var/folders/3l/t6ndp46d3sz8nb2_lxvxc6rw0000gn/T/ipykernel_91783/4290611751.py:45 train_step  *\n        _critic_x_losses = critic_x_loss(generator, critic_x, critic_x_loss_fn, gradient_penalty_weight , x_mb, z, valid, fake, minibatch_size)\n    /var/folders/3l/t6ndp46d3sz8nb2_lxvxc6rw0000gn/T/ipykernel_91783/2170213254.py:28 critic_x_loss  *\n        critic_x_grad_penalty = critic_x_gradient_penalty(mini_batch_size, x_mb, x_)\n\n    TypeError: tf__critic_x_gradient_penalty() missing 1 required positional argument: 'y_pred'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3l/t6ndp46d3sz8nb2_lxvxc6rw0000gn/T/ipykernel_91783/2500881765.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mold_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mold_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                 \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iterations_critic\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcritic_x\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mcritic_z\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m                 \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/tadGAN_tf2/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/tadGAN_tf2/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/tadGAN_tf2/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 763\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    764\u001b[0m             *args, **kwds))\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/tadGAN_tf2/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/tadGAN_tf2/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/tadGAN_tf2/venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3279\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/tadGAN_tf2/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/tadGAN_tf2/venv/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/tadGAN_tf2/venv/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    /var/folders/3l/t6ndp46d3sz8nb2_lxvxc6rw0000gn/T/ipykernel_91783/4290611751.py:45 train_step  *\n        _critic_x_losses = critic_x_loss(generator, critic_x, critic_x_loss_fn, gradient_penalty_weight , x_mb, z, valid, fake, minibatch_size)\n    /var/folders/3l/t6ndp46d3sz8nb2_lxvxc6rw0000gn/T/ipykernel_91783/2170213254.py:28 critic_x_loss  *\n        critic_x_grad_penalty = critic_x_gradient_penalty(mini_batch_size, x_mb, x_)\n\n    TypeError: tf__critic_x_gradient_penalty() missing 1 required positional argument: 'y_pred'\n"
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "save_dir = \"models/\"\n",
    "with trange(epoch , position=0 , unit=\"epoch\") as pbar:\n",
    "        for epoch in pbar:\n",
    "            pbar.set_description(f\"Epoch {epoch}\")\n",
    "            for step , data in enumerate(dataset):\n",
    "                data = list(data.values())\n",
    "                data = np.array(data)\n",
    "                old_shape = data.shape\n",
    "                data = data.reshape(old_shape[1] , old_shape[0])\n",
    "                loss_dict = train_step(data, n_iterations_critic , encoder , generator , critic_x , critic_z)\n",
    "                pbar.set_postfix(loss_dict , refresh=True)\n",
    "            \n",
    "            if epoch % 5 == 0:\n",
    "                encoder.save_weights(encoder_checkpoint.format(epoch))\n",
    "                generator.save_weights(generator_checkpoint.format(epoch))\n",
    "                critic_x.save_weights(critic_x_checkpoint.format(epoch))\n",
    "                critic_z.save_weights(critic_z_checkpoint.format(epoch))\n",
    "                \n",
    "        \n",
    "        encoder.save(\"models/encoder\")\n",
    "        generator.save(\"models/generator\")\n",
    "        critic_x.save(\"models/critic_x\")\n",
    "        critic_z.save(\"model/critic_z\")\n",
    "'''\n",
    "tadGan = TadGAN()\n",
    "print(tadGan.encoder.summary())\n",
    "print(tadGan.generator.summary())\n",
    "print(tadGan.critic_x.summary())\n",
    "print(tadGan.critic_z.summary())\n",
    "tadGan.summary()\n",
    "'''\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1b7e350b82617662beaaacd567468af7b9ee2240126ae9c7aedafcf99e0b4bb"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
